{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74952bc1-9a7c-414a-b6ae-f7f1b5e1effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "import tqdm\n",
    "\n",
    "from torch.optim import SGD,Adam\n",
    "from torch.nn import MSELoss\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e49f1-be2f-4fb1-b044-91b7fab071bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = 'http://files.grouplens.org/datasets/movielens/ml-100k.zip'\n",
    "DATASET_ARCHIVE = 'ml-100k.zip'\n",
    "\n",
    "request.urlretrieve(DATASET_URL, DATASET_ARCHIVE)\n",
    "with zipfile.ZipFile(DATASET_ARCHIVE) as archive:\n",
    "    archive.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662ba9e6-54da-4376-b9da-67846defd1c5",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "1. Go over the NCF paper (https://arxiv.org/abs/1708.05031) to understand the architecture of the model\n",
    "2. Implement NCF class. In particular:  \n",
    "    a. implement __init__ to create the model achtecture  \n",
    "    b. initialisation is already implemented for you  \n",
    "    c. implement forward. Forward should take u_id and i_id and return the propability of consumption given u_id and i_id (what is the output layer?)\n",
    "    d. Test your forward by providing some random u_id and i_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7af85a-336b-4424-aefd-22814eac48ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, nb_users, nb_items,\n",
    "                 mf_dim, mlp_layer_sizes, dropout=0):\n",
    "        \n",
    "        if mlp_layer_sizes[0] % 2 != 0:\n",
    "            raise RuntimeError('u dummy, mlp_layer_sizes[0] % 2 != 0')\n",
    "        super(NeuMF, self).__init__()\n",
    "        nb_mlp_layers = len(mlp_layer_sizes)\n",
    "        #YOUR TASK: ADD EMBEEDING LAYERS TO THE MODEL - be careful with sizes ~5 lines of code\n",
    "\n",
    "        \n",
    "        #YOUR TASK: ADD MLPs and the final layer ~4-6 lines of code\n",
    "        \n",
    "        \n",
    "        #YOUR TASK: Initialze weights ~4 lines of code - use glorot (final MLP has to use lecunn\n",
    "\n",
    "        def glorot_uniform(layer):\n",
    "            fan_in, fan_out = layer.in_features, layer.out_features\n",
    "            limit = np.sqrt(6. / (fan_in + fan_out))\n",
    "            layer.weight.data.uniform_(-limit, limit)\n",
    "\n",
    "        def lecunn_uniform(layer):\n",
    "            fan_in, fan_out = layer.in_features, layer.out_features  # noqa: F841, E501\n",
    "            limit = np.sqrt(3. / fan_in)\n",
    "            layer.weight.data.uniform_(-limit, limit)\n",
    "        for layer in self.mlp:\n",
    "            if type(layer) != nn.Linear:\n",
    "                continue\n",
    "            glorot_uniform(layer)\n",
    "        lecunn_uniform(self.final)\n",
    "\n",
    "    def forward(self, user, item, sigmoid=False):\n",
    "        #YOUR TASK: Implement model forward. ~15 lines of code\n",
    "        \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb79876-648f-4ca7-8747-30a549a594c8",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "1. Implement ML100kDataset. In particular:  \n",
    "    a. implement all needed functions that overload PyTorch Dataset\n",
    "    b. Implement negaive sampling. This can be approximated negative sampling negative_samples=n means that if user A has m training points (positive) it will have n*m negive points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6b869a-f634-40fe-b1c8-c1438017a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML100kDataset(Dataset):\n",
    "    def __init__(self,file_name,negative_samples=1):\n",
    "        ratings_df=pd.read_csv(file_name, header=None, names=['user_id', 'item_id', 'rating', 'timestamp'],delim_whitespace=True)\n",
    "        max_item = np.max(ratings_df['item_id'])\n",
    "        ratings_df.drop(['rating','timestamp'],axis=1,inplace=True)\n",
    "        ratings = torch.from_numpy(ratings_df.values)         \n",
    "        self.negative_samples = negative_samples\n",
    "        self.raw_dataset_length = len(ratings_df.index) \n",
    "        self.length_after_augmentation = self.raw_dataset_length * (self.negative_samples + 1)\n",
    "        #YOUR TASK: Implement negative sampling. ~7 lines of code. NOTE: the dataset does not have to be shuffled, but you can do this! store them in self.users,self.items,self.labels\n",
    "        #Sample neg users\n",
    "        \n",
    "        #sample neg items\n",
    "        \n",
    "        #labels\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_after_augmentation\n",
    "  \n",
    "    def __getitem__(self,idx):\n",
    "        return self.users[idx],self.items[idx],self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f688447-fa62-4aa4-a598-e132fd8918e2",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "1. Implement training loop. In particular:  \n",
    "    a. Every epoch you make log average train loss and test/eval loss  \n",
    "    b. use different loss/optimiser/other HP. Use dependency injection to play with them.  \n",
    "    c. Find \"best\" HP during cross-validation.  \n",
    "    d. Check when model overfits with learning curves.  \n",
    "    e. What are the conclusions.  \n",
    "    f*. You can user HR@n from previous excersise to look for overfitting.  \n",
    "    g*. Check the influence of shuffling on your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75100a8a-4c7a-475a-b71f-d0eefba047e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR TASK: Implement entire training loop for the model with selection of HPs ~25+ lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a39045-18d7-470d-8bae-99105a27c5d7",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e927a-44e4-4504-b24f-dbf1cc501ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss,label='train_loss vs epoch')\n",
    "plt.plot(test_loss,label='test_loss vs epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
